% This file was created with JabRef 2.8.
% Encoding: ISO8859_1

@ARTICLE{Anders2013,
  author = {Anders, Simon and McCarthy, Davis J and Chen, Yunshun and Okoniewski,
	Michal and Smyth, Gordon K and Huber, Wolfgang and Robinson, Mark
	D},
  title = {Count-based differential expression analysis of {RNA} sequencing
	data using {R} and {Bioconductor}},
  journal = {Nat. Protocols},
  year = {2013},
  volume = {8},
  pages = {1765--1786},
  number = {9},
  month = {09},
  bdsk-url-1 = {http://dx.doi.org/10.1038/nprot.2013.099},
  date = {2013/09//print},
  date-added = {2013-09-09 00:09:47 +0000},
  date-modified = {2014-01-03 04:01:30 +0000},
  isbn = {1754-2189},
  l3 = {10.1038/nprot.2013.099; http://www.nature.com/nprot/journal/v8/n9/abs/nprot.2013.099.html#supplementary-information},
  m3 = {Protocol},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited.
	All Rights Reserved.},
  ty = {JOUR},
  url = {http://dx.doi.org/10.1038/nprot.2013.099}
}

@ARTICLE{Angiuoli2011,
  author = {Angiuoli, Samuel V. and Matalka, Malcolm and Gussman, Aaron and Galens,
	Kevin and Vangala, Mahesh and Riley, David R. and Arze, Cesar and
	White, James R. and White, Owen and Fricke, W Florian},
  title = {CloVR: a virtual machine for automated and portable sequence analysis
	from the desktop using cloud computing.},
  journal = {BMC Bioinformatics},
  year = {2011},
  volume = {12},
  pages = {356},
  abstract = {Next-generation sequencing technologies have decentralized sequence
	acquisition, increasing the demand for new bioinformatics tools that
	are easy to use, portable across multiple platforms, and scalable
	for high-throughput applications. Cloud computing platforms provide
	on-demand access to computing infrastructure over the Internet and
	can be used in combination with custom built virtual machines to
	distribute pre-packaged with pre-configured software.We describe
	the Cloud Virtual Resource, CloVR, a new desktop application for
	push-button automated sequence analysis that can utilize cloud computing
	resources. CloVR is implemented as a single portable virtual machine
	(VM) that provides several automated analysis pipelines for microbial
	genomics, including 16S, whole genome and metagenome sequence analysis.
	The CloVR VM runs on a personal computer, utilizes local computer
	resources and requires minimal installation, addressing key challenges
	in deploying bioinformatics workflows. In addition CloVR supports
	use of remote cloud computing resources to improve performance for
	large-scale sequence processing. In a case study, we demonstrate
	the use of CloVR to automatically process next-generation sequencing
	data on multiple cloud computing platforms.The CloVR VM and associated
	architecture lowers the barrier of entry for utilizing complex analysis
	protocols on both local single- and multi-core computers and cloud
	systems for high throughput data processing.},
  bdsk-url-1 = {http://dx.doi.org/10.1186/1471-2105-12-356},
  doi = {10.1186/1471-2105-12-356},
  institution = {Institute for Genome Sciences (IGS), University of Maryland School
	of Medicine, Baltimore, Maryland, USA. angiuoli@umiacs.umd.edu},
  keywords = {Computational Biology; Computers; Genomics; High-Throughput Nucleotide
	Sequencing; Internet; Sequence Analysis, DNA; Software},
  language = {eng},
  medline-pst = {epublish},
  owner = {bau04c},
  pii = {1471-2105-12-356},
  pmid = {21878105},
  timestamp = {2013.08.28},
  url = {http://dx.doi.org/10.1186/1471-2105-12-356}
}

@ARTICLE{Auer2010,
  author = {Auer, Paul L. and Doerge, R. W.},
  title = {Statistical design and analysis of {RNA} sequencing data.},
  journal = {Genetics},
  year = {2010},
  volume = {185},
  pages = {405--416},
  number = {2},
  month = {Jun},
  abstract = {Next-generation sequencing technologies are quickly becoming the preferred
	approach for characterizing and quantifying entire genomes. Even
	though data produced from these technologies are proving to be the
	most informative of any thus far, very little attention has been
	paid to fundamental design aspects of data collection and analysis,
	namely sampling, randomization, replication, and blocking. We discuss
	these concepts in an RNA sequencing framework. Using simulations
	we demonstrate the benefits of collecting replicated RNA sequencing
	data according to well known statistical designs that partition the
	sources of biological and technical variation. Examples of these
	designs and their corresponding models are presented with the goal
	of testing differential expression.},
  bdsk-url-1 = {http://dx.doi.org/10.1534/genetics.110.114983},
  date-modified = {2014-01-03 04:02:04 +0000},
  doi = {10.1534/genetics.110.114983},
  institution = {Department of Statistics, Purdue University, West Lafayette, Indiana
	47907, USA.},
  keywords = {Base Sequence; Clinical Laboratory Techniques; Research Design},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {genetics.110.114983},
  pmid = {20439781},
  timestamp = {2013.08.27},
  url = {http://dx.doi.org/10.1534/genetics.110.114983}
}

@ARTICLE{Brouwer2012,
  author = {Brouwer, R W W. and {van den Hout}, M C G N. and Grosveld, F. G.
	and {van Ijcken}, W F J.},
  title = {NARWHAL, a primary analysis pipeline for NGS data.},
  journal = {Bioinformatics},
  year = {2012},
  volume = {28},
  pages = {284--285},
  number = {2},
  month = {Jan},
  abstract = {The NARWHAL software pipeline has been developed to automate the primary
	analysis of Illumina sequencing data. This pipeline combines a new
	and flexible de-multiplexing tool with open-source aligners and automated
	quality assessment. The entire pipeline can be run using only one
	simple sample-sheet for diverse sequencing applications. NARWHAL
	creates a sample-oriented data structure and outperforms existing
	tools in speed.https://trac.nbic.nl/narwhal/.},
  bdsk-url-1 = {http://dx.doi.org/10.1093/bioinformatics/btr613},
  doi = {10.1093/bioinformatics/btr613},
  institution = {Center for Biomics, Department of Cell Biology, Erasmus Medical Center,
	Rotterdam, The Netherlands.},
  keywords = {High-Throughput Nucleotide Sequencing; Humans; Oligonucleotide Array
	Sequence Analysis; Sequence Alignment; Sequence Analysis, DNA, methods;
	Software},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {btr613},
  pmid = {22072383},
  timestamp = {2013.08.23},
  url = {http://dx.doi.org/10.1093/bioinformatics/btr613}
}

@ARTICLE{Cieslik2011,
  author = {Cieslik, Marcin and Mura, Cameron},
  title = {A lightweight, flow-based toolkit for parallel and distributed bioinformatics
	pipelines.},
  journal = {BMC Bioinformatics},
  year = {2011},
  volume = {12},
  pages = {61},
  abstract = {Bioinformatic analyses typically proceed as chains of data-processing
	tasks. A pipeline, or 'workflow', is a well-defined protocol, with
	a specific structure defined by the topology of data-flow interdependencies,
	and a particular functionality arising from the data transformations
	applied at each step. In computer science, the dataflow programming
	(DFP) paradigm defines software systems constructed in this manner,
	as networks of message-passing components. Thus, bioinformatic workflows
	can be naturally mapped onto DFP concepts.To enable the flexible
	creation and execution of bioinformatics dataflows, we have written
	a modular framework for parallel pipelines in Python ('PaPy'). A
	PaPy workflow is created from re-usable components connected by data-pipes
	into a directed acyclic graph, which together define nested higher-order
	map functions. The successive functional transformations of input
	data are evaluated on flexibly pooled compute resources, either local
	or remote. Input items are processed in batches of adjustable size,
	all flowing one to tune the trade-off between parallelism and lazy-evaluation
	(memory consumption). An add-on module ('NuBio') facilitates the
	creation of bioinformatics workflows by providing domain specific
	data-containers (e.g., for biomolecular sequences, alignments, structures)
	and functionality (e.g., to parse/write standard file formats).PaPy
	offers a modular framework for the creation and deployment of parallel
	and distributed data-processing workflows. Pipelines derive their
	functionality from user-written, data-coupled components, so PaPy
	also can be viewed as a lightweight toolkit for extensible, flow-based
	bioinformatics data-processing. The simplicity and flexibility of
	distributed PaPy pipelines may help users bridge the gap between
	traditional desktop/workstation and grid computing. PaPy is freely
	distributed as open-source Python code at http://muralab.org/PaPy,
	and includes extensive documentation and annotated usage examples.},
  bdsk-url-1 = {http://dx.doi.org/10.1186/1471-2105-12-61},
  doi = {10.1186/1471-2105-12-61},
  institution = {Department of Chemistry, University of Virginia, Charlottesville,
	VA 22904-4319, USA.},
  keywords = {Computational Biology, methods; Computing Methodologies; Programming
	Languages; Software; Workflow},
  language = {eng},
  medline-pst = {epublish},
  owner = {bau04c},
  pii = {1471-2105-12-61},
  pmid = {21352538},
  timestamp = {2013.08.23},
  url = {http://dx.doi.org/10.1186/1471-2105-12-61}
}

@ARTICLE{Queue,
  author = {{GATK Development Team}},
  title = {Genome Analysis Toolkit},
  journal = {{\url{https://github.com/broadgsa/gatk/}}},
  year = {{Accessed 26th September 2013} },
  date-modified = {2014-01-03 04:47:42 +0000},
  owner = {bau04c},
  timestamp = {2013.09.26}
}

@ARTICLE{Goecks2010,
  author = {Goecks, Jeremy and Nekrutenko, Anton and Taylor, James and {Galaxy
	Team}},
  title = {Galaxy: a comprehensive approach for supporting accessible, reproducible,
	and transparent computational research in the life sciences.},
  journal = {Genome Biol},
  year = {2010},
  volume = {11},
  pages = {R86},
  number = {8},
  abstract = {Increased reliance on computational approaches in the life sciences
	has revealed grave concerns about how accessible and reproducible
	computation-reliant results truly are. Galaxy http://usegalaxy.org,
	an open web-based platform for genomic research, addresses these
	problems. Galaxy automatically tracks and manages data provenance
	and provides support for capturing the context and intent of computational
	methods. Galaxy Pages are interactive, web-based documents that provide
	users with a medium to communicate a complete computational analysis.},
  bdsk-url-1 = {http://dx.doi.org/10.1186/gb-2010-11-8-r86},
  date-modified = {2014-01-03 04:12:31 +0000},
  doi = {10.1186/gb-2010-11-8-r86},
  institution = {Department of Biology and Department of Mathematics and Computer
	Science, Emory University, 1510 Clifton Road NE, Atlanta, GA 30322,
	USA. jeremy.goecks@emory.edu},
  keywords = {Algorithms; Animals; Computational Biology, methods; Databases, Nucleic
	Acid; Genomics, methods; Humans; Internet},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {gb-2010-11-8-r86},
  pmid = {20738864},
  timestamp = {2013.08.23},
  url = {http://dx.doi.org/10.1186/gb-2010-11-8-r86}
}

@ARTICLE{Goodstadt2010,
  author = {Goodstadt, Leo},
  title = {Ruffus: a lightweight Python library for computational pipelines.},
  journal = {Bioinformatics},
  year = {2010},
  volume = {26},
  pages = {2778--2779},
  number = {21},
  month = {Nov},
  abstract = {Computational pipelines are common place in scientific research. However,
	most of the resources for constructing pipelines are heavyweight
	systems with graphical user interfaces. Ruffus is a library for the
	creation of computational pipelines. Its lightweight and unobtrusive
	design recommends it for use even for the most trivial of analyses.
	At the same time, it is powerful enough to have been used for complex
	workflows involving more than 50 interdependent stages. Availability
	and implementation: Ruffus is written in python. Source code, a short
	tutorial, examples and a comprehensive user manual are freely available
	at http://www.ruffus.org.uk. The example program is available at
	http://www.ruffus.org.uk/examples/bioinformatics},
  bdsk-url-1 = {http://dx.doi.org/10.1093/bioinformatics/btq524},
  doi = {10.1093/bioinformatics/btq524},
  institution = {Medical Research Council Functional Genomics Unit, Department of
	Physiology, Anatomy and Genetics, University of Oxford, Oxford, UK.
	ruffus@llew.org.uk},
  keywords = {Computational Biology, methods; Databases, Factual; Internet; Software},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {btq524},
  pmid = {20847218},
  timestamp = {2013.08.23},
  url = {http://dx.doi.org/10.1093/bioinformatics/btq524}
}

@ARTICLE{Halbritter2012,
  author = {Halbritter, Florian and Vaidya, Harsh J. and Tomlinson, Simon R.},
  title = {GeneProf: analysis of high-throughput sequencing experiments.},
  journal = {Nat Methods},
  year = {2012},
  volume = {9},
  pages = {7--8},
  number = {1},
  month = {Jan},
  bdsk-url-1 = {http://dx.doi.org/10.1038/nmeth.1809},
  doi = {10.1038/nmeth.1809},
  keywords = {Capillary Electrochromatography; Computational Biology, methods; Gene
	Expression; Sequence Alignment; Sequence Analysis, DNA, methods;
	Sequence Analysis, RNA, methods; Software},
  language = {eng},
  medline-pst = {epublish},
  owner = {bau04c},
  pii = {nmeth.1809},
  pmid = {22205509},
  timestamp = {2013.08.28},
  url = {http://dx.doi.org/10.1038/nmeth.1809}
}

@ARTICLE{Hoon2003,
  author = {Hoon, Shawn and Ratnapu, Kiran Kumar and Chia, Jer-Ming and Kumarasamy,
	Balamurugan and Juguang, Xiao and Clamp, Michele and Stabenau, Arne
	and Potter, Simon and Clarke, Laura and Stupka, Elia},
  title = {Biopipe: a flexible framework for protocol-based bioinformatics analysis.},
  journal = {Genome Res},
  year = {2003},
  volume = {13},
  pages = {1904--1915},
  number = {8},
  month = {Aug},
  abstract = {We identify several challenges facing bioinformatics analysis today.
	Firstly, to fulfill the promise of comparative studies, bioinformatics
	analysis will need to accommodate different sources of data residing
	in a federation of databases that, in turn, come in different formats
	and modes of accessibility. Secondly, the tsunami of data to be handled
	will require robust systems that enable bioinformatics analysis to
	be carried out in a parallel fashion. Thirdly, the ever-evolving
	state of bioinformatics presents new algorithms and paradigms in
	conducting analysis. This means that any bioinformatics framework
	must be flexible and generic enough to accommodate such changes.
	In addition, we identify the need for introducing an explicit protocol-based
	approach to bioinformatics analysis that will lend rigorousness to
	the analysis. This makes it easier for experimentation and replication
	of results by external parties. Biopipe is designed in an effort
	to meet these goals. It aims to allow researchers to focus on protocol
	design. At the same time, it is designed to work over a compute farm
	and thus provides high-throughput performance. A common exchange
	format that encapsulates the entire protocol in terms of the analysis
	modules, parameters, and data versions has been developed to provide
	a powerful way in which to distribute and reproduce results. This
	will enable researchers to discuss and interpret the data better
	as the once implicit assumptions are now explicitly defined within
	the Biopipe framework.},
  bdsk-url-1 = {http://dx.doi.org/10.1101/gr.1363103},
  doi = {10.1101/gr.1363103},
  institution = {Institute of Molecular and Cell Biology, National University of Singapore,
	Singapore 117609.},
  keywords = {Amino Acid Sequence; Animals; Computational Biology, methods; Databases,
	Protein; Drosophila Proteins, genetics; Humans; Phylogeny; Proteins,
	genetics; Research Design, trends; Software; Software Design; Takifugu,
	genetics},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {1363103},
  pmid = {12869579},
  timestamp = {2013.08.28},
  url = {http://dx.doi.org/10.1101/gr.1363103}
}

@ARTICLE{Koester2012,
  author = {K\"oster, Johannes and Rahmann, Sven},
  title = {Snakemake -- a scalable bioinformatics workflow engine.},
  journal = {Bioinformatics},
  year = {2012},
  volume = {28},
  pages = {2520--2522},
  number = {19},
  month = {Oct},
  abstract = {Snakemake is a workflow engine that provides a readable Python-based
	workflow definition language and a powerful execution environment
	that scales from single-core workstations to compute clusters without
	modifying the workflow. It is the first system to support the use
	of automatically inferred multiple named wildcards (or variables)
	in input and output filenames.http://snakemake.googlecode.com.johannes.koester@uni-due.de.},
  bdsk-url-1 = {http://dx.doi.org/10.1093/bioinformatics/bts480},
  date-modified = {2014-01-03 04:03:19 +0000},
  doi = {10.1093/bioinformatics/bts480},
  institution = {Genome Informatics, Institute of Human Genetics, University of Duisburg-Essen
	and Paediatric Oncology, University Childrens Hospital, 45147 Essen,
	Germany. johannes.koester@uni-due.de},
  keywords = {Automatic Data Processing; Computational Biology, methods; Programming
	Languages; Software; Workflow},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {bts480},
  pmid = {22908215},
  timestamp = {2013.08.28},
  url = {http://dx.doi.org/10.1093/bioinformatics/bts480}
}

@ARTICLE{Langmead2012,
  author = {Langmead, Ben and Salzberg, Steven L.},
  title = {Fast gapped-read alignment with Bowtie 2.},
  journal = {Nat Methods},
  year = {2012},
  volume = {9},
  pages = {357--359},
  number = {4},
  month = {Apr},
  abstract = {As the rate of sequencing increases, greater throughput is demanded
	from read aligners. The full-text minute index is often used to make
	alignment very fast and memory-efficient, but the approach is ill-suited
	to finding longer, gapped alignments. Bowtie 2 combines the strengths
	of the full-text minute index with the flexibility and speed of hardware-accelerated
	dynamic programming algorithms to achieve a combination of high speed,
	sensitivity and accuracy.},
  bdsk-url-1 = {http://dx.doi.org/10.1038/nmeth.1923},
  doi = {10.1038/nmeth.1923},
  institution = {Center for Bioinformatics and Computational Biology, Institute for
	Advanced Computer Studies, University of Maryland, College Park,
	Maryland, USA. blangmea@jhsph.edu},
  keywords = {Algorithms; Computational Biology, methods; Databases, Genetic; Genome,
	Human, genetics; Humans; Sequence Alignment, methods; Sequence Analysis,
	DNA, methods},
  language = {eng},
  medline-pst = {epublish},
  owner = {fabianbuske},
  pii = {nmeth.1923},
  pmid = {22388286},
  timestamp = {2013.09.24},
  url = {http://dx.doi.org/10.1038/nmeth.1923}
}

@ARTICLE{Langmead2009,
  author = {Langmead, Ben and Schatz, Michael C. and Lin, Jimmy and Pop, Mihai
	and Salzberg, Steven L.},
  title = {Searching for SNPs with cloud computing.},
  journal = {Genome Biol},
  year = {2009},
  volume = {10},
  pages = {R134},
  number = {11},
  abstract = {As DNA sequencing outpaces improvements in computer speed, there is
	a critical need to accelerate tasks like alignment and SNP calling.
	Crossbow is a cloud-computing software tool that combines the aligner
	Bowtie and the SNP caller SOAPsnp. Executing in parallel using Hadoop,
	Crossbow analyzes data comprising 38-fold coverage of the human genome
	in three hours using a 320-CPU cluster rented from a cloud computing
	service for about $85. Crossbow is available from http://bowtie-bio.sourceforge.net/crossbow/.},
  bdsk-url-1 = {http://dx.doi.org/10.1186/gb-2009-10-11-r134},
  doi = {10.1186/gb-2009-10-11-r134},
  institution = {Department of Biostatistics, Johns Hopkins Bloomberg School of Public
	Health, 615 North Wolfe Street, Baltimore, Maryland 21205, USA. blangmea@jhsph.edu},
  keywords = {Algorithms; Alleles; Chromosomes, Human, Pair 22, genetics; Chromosomes,
	Human, X, genetics; Chromosomes, ultrastructure; Computational Biology,
	methods; Computer Simulation; Computers; Heterozygote; Humans; Models,
	Genetic; Polymorphism, Single Nucleotide; Sequence Analysis, DNA;
	Software},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {gb-2009-10-11-r134},
  pmid = {19930550},
  timestamp = {2013.08.28},
  url = {http://dx.doi.org/10.1186/gb-2009-10-11-r134}
}

@ARTICLE{Li2009,
  author = {Li, Heng and Durbin, Richard},
  title = {Fast and accurate short read alignment with Burrows-Wheeler transform.},
  journal = {Bioinformatics},
  year = {2009},
  volume = {25},
  pages = {1754--1760},
  number = {14},
  month = {Jul},
  abstract = {The enormous amount of short reads generated by the new DNA sequencing
	technologies call for the development of fast and accurate read alignment
	programs. A first generation of hash table-based methods has been
	developed, including MAQ, which is accurate, feature rich and fast
	enough to align short reads from a single individual. However, MAQ
	does not support gapped alignment for single-end reads, which makes
	it unsuitable for alignment of longer reads where indels may occur
	frequently. The speed of MAQ is also a concern when the alignment
	is scaled up to the resequencing of hundreds of individuals.We implemented
	Burrows-Wheeler Alignment tool (BWA), a new read alignment package
	that is based on backward search with Burrows-Wheeler Transform (BWT),
	to efficiently align short sequencing reads against a large reference
	sequence such as the human genome, allowing mismatches and gaps.
	BWA supports both base space reads, e.g. from Illumina sequencing
	machines, and color space reads from AB SOLiD machines. Evaluations
	on both simulated and real data suggest that BWA is approximately
	10-20x faster than MAQ, while achieving similar accuracy. In addition,
	BWA outputs alignment in the new standard SAM (Sequence Alignment/Map)
	format. Variant calling and other downstream analyses after the alignment
	can be achieved with the open source SAMtools software package.http://maq.sourceforge.net.},
  bdsk-url-1 = {http://dx.doi.org/10.1093/bioinformatics/btp324},
  doi = {10.1093/bioinformatics/btp324},
  institution = {Wellcome Trust Sanger Institute, Wellcome Trust Genome Campus, Cambridge,
	CB10 1SA, UK.},
  keywords = {Algorithms; Genomics, methods; Sequence Alignment, methods; Sequence
	Analysis, DNA, methods; Software},
  language = {eng},
  medline-pst = {ppublish},
  owner = {fabianbuske},
  pii = {btp324},
  pmid = {19451168},
  timestamp = {2013.09.24},
  url = {http://dx.doi.org/10.1093/bioinformatics/btp324}
}

@ARTICLE{Lindenbaum2011,
  author = {Lindenbaum, Pierre and {Le Scouarnec}, Solena and Portero, Vincent
	and Redon, Richard},
  title = {Knime4Bio: a set of custom nodes for the interpretation of next-generation
	sequencing data with KNIME.},
  journal = {Bioinformatics},
  year = {2011},
  volume = {27},
  pages = {3200--3201},
  number = {22},
  month = {Nov},
  abstract = {Analysing large amounts of data generated by next-generation sequencing
	(NGS) technologies is difficult for researchers or clinicians without
	computational skills. They are often compelled to delegate this task
	to computer biologists working with command line utilities. The availability
	of easy-to-use tools will become essential with the generalization
	of NGS in research and diagnosis. It will enable investigators to
	handle much more of the analysis. Here, we describe Knime4Bio, a
	set of custom nodes for the KNIME (The Konstanz Information Miner)
	interactive graphical workbench, for the interpretation of large
	biological datasets. We demonstrate that this tool can be utilized
	to quickly retrieve previously published scientific findings.},
  bdsk-url-1 = {http://dx.doi.org/10.1093/bioinformatics/btr554},
  doi = {10.1093/bioinformatics/btr554},
  institution = {Institut du thorax, Inserm UMR 915, Centre Hospitalier Universitaire
	de Nantes, 44000 Nantes, France.},
  keywords = {Hajdu-Cheney Syndrome, genetics; High-Throughput Nucleotide Sequencing,
	methods; Humans; Software},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {btr554},
  pmid = {21984761},
  timestamp = {2013.08.23},
  url = {http://dx.doi.org/10.1093/bioinformatics/btr554}
}

@ARTICLE{Linke2011,
  author = {Linke, Burkhard and Giegerich, Robert and Goesmann, Alexander},
  title = {Conveyor: a workflow engine for bioinformatic analyses.},
  journal = {Bioinformatics},
  year = {2011},
  volume = {27},
  pages = {903--911},
  number = {7},
  month = {Apr},
  abstract = {The rapidly increasing amounts of data available from new high-throughput
	methods have made data processing without automated pipelines infeasible.
	As was pointed out in several publications, integration of data and
	analytic resources into workflow systems provides a solution to this
	problem, simplifying the task of data analysis. Various applications
	for defining and running workflows in the field of bioinformatics
	have been proposed and published, e.g. Galaxy, Mobyle, Taverna, Pegasus
	or Kepler. One of the main aims of such workflow systems is to enable
	scientists to focus on analysing their datasets instead of taking
	care for data management, job management or monitoring the execution
	of computational tasks. The currently available workflow systems
	achieve this goal, but fundamentally differ in their way of executing
	workflows.We have developed the Conveyor software library, a multitiered
	generic workflow engine for composition, execution and monitoring
	of complex workflows. It features an open, extensible system architecture
	and concurrent program execution to exploit resources available on
	modern multicore CPU hardware. It offers the ability to build complex
	workflows with branches, loops and other control structures. Two
	example use cases illustrate the application of the versatile Conveyor
	engine to common bioinformatics problems.The Conveyor application
	including client and server are available at http://conveyor.cebitec.uni-bielefeld.de.},
  bdsk-url-1 = {http://dx.doi.org/10.1093/bioinformatics/btr040},
  doi = {10.1093/bioinformatics/btr040},
  institution = {Bioinformatics Resource Faciliy, Center for Biotechnology and Faculty
	of Technology, Bielefeld University, 33615 Bielefeld, Germany. blinke@ceBiTec.Uni-Bielefeld.De},
  keywords = {Computational Biology; Escherichia coli, genetics; Genome, Bacterial;
	Genomics; Molecular Sequence Annotation; Software; Workflow},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {btr040},
  pmid = {21278189},
  timestamp = {2013.08.23},
  url = {http://dx.doi.org/10.1093/bioinformatics/btr040}
}

@ARTICLE{McCoy2013,
  author = {McCoy, Connor O. and Gallagher, Aaron and Hoffman, Noah G. and Matsen,
	Frederick A.},
  title = {Nestly -- a framework for running software with nested parameter
	choices and aggregating results.},
  journal = {Bioinformatics},
  year = {2013},
  volume = {29},
  pages = {387--388},
  number = {3},
  month = {Feb},
  abstract = {The execution of a software application or pipeline using various
	combinations of parameters and inputs is a common task in bioinformatics.
	In the absence of a specialized tool to organize, streamline and
	formalize this process, scientists must write frequently complex
	scripts to perform these tasks. We present nestly, a Python package
	to facilitate running tools with nested combinations of parameters
	and inputs. nestly provides three components. First, a module to
	build nested directory structures corresponding to choices of parameters.
	Second, the nestrun script to run a given command using each set
	of parameter choices. Third, the nestagg script to aggregate results
	of the individual runs into a CSV file, as well as support for more
	complex aggregation. We also include a module for easily specifying
	nested dependencies for the SCons build tool, enabling incremental
	builds.Source, documentation and tutorial examples are available
	at http://github.com/fhcrc/nestly. nestly can be installed from the
	Python Package Index via pip; it is open source (MIT license).},
  bdsk-url-1 = {http://dx.doi.org/10.1093/bioinformatics/bts696},
  date-modified = {2014-01-03 04:03:50 +0000},
  doi = {10.1093/bioinformatics/bts696},
  institution = {Program in Computational Biology, Fred Hutchinson Cancer Research
	Center, Seattle, WA 98109, USA. cmccoy@fhcrc.org},
  keywords = {Algorithms; Computational Biology; Humans; Software},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {bts696},
  pmid = {23220574},
  timestamp = {2013.08.28},
  url = {http://dx.doi.org/10.1093/bioinformatics/bts696}
}

@ARTICLE{Queue2,
  author = {McKenna, Aaron and Hanna, Matthew and Banks, Eric and Sivachenko,
	Andrey and Cibulskis, Kristian and Kernytsky, Andrew and Garimella,
	Kiran and Altshuler, David and Gabriel, Stacey and Daly, Mark and
	DePristo, Mark A},
  title = {{The Genome Analysis Toolkit: a MapReduce framework for analyzing
	next-generation DNA sequencing data.}},
  journal = {Genome Res},
  year = {2010},
  volume = {20},
  pages = {1297--1303},
  number = {9},
  month = {Sep},
  abstract = {Next-generation DNA sequencing (NGS) projects, such as the 1000 Genomes
	Project, are already revolutionizing our understanding of genetic
	variation among individuals. However, the massive data sets generated
	by NGS--the 1000 Genome pilot alone includes nearly five terabases--make
	writing feature-rich, efficient, and robust analysis tools difficult
	for even computationally sophisticated individuals. Indeed, many
	professionals are limited in the scope and the ease with which they
	can answer scientific questions by the complexity of accessing and
	manipulating the data produced by these machines. Here, we discuss
	our Genome Analysis Toolkit (GATK), a structured programming framework
	designed to ease the development of efficient and robust analysis
	tools for next-generation DNA sequencers using the functional programming
	philosophy of MapReduce. The GATK provides a small but rich set of
	data access patterns that encompass the majority of analysis tool
	needs. Separating specific analysis calculations from common data
	management infrastructure enables us to optimize the GATK framework
	for correctness, stability, and CPU and memory efficiency and to
	enable distributed and shared memory parallelization. We highlight
	the capabilities of the GATK by describing the implementation and
	application of robust, scale-tolerant tools like coverage calculators
	and single nucleotide polymorphism (SNP) calling. We conclude that
	the GATK programming framework enables developers and analysts to
	quickly and easily write efficient and robust NGS tools, many of
	which have already been incorporated into large-scale sequencing
	projects like the 1000 Genomes Project and The Cancer Genome Atlas.},
  address = {Program in Medical and Population Genetics, The Broad Institute of
	Harvard and MIT, Cambridge, Massachusetts 02142, USA.},
  bdsk-url-1 = {http://dx.doi.org/10.1101/gr.107524.110},
  crdt = {2010/07/21 06:00},
  da = {20100902},
  date = {2010 Sep},
  date-added = {2014-01-03 04:56:56 +0000},
  date-modified = {2014-01-03 04:57:32 +0000},
  dcom = {20101223},
  dep = {20100719},
  doi = {10.1101/gr.107524.110},
  edat = {2010/07/21 06:00},
  gr = {54 HG003067/HG/NHGRI NIH HHS/United States; U01 HG005208/HG/NHGRI
	NIH HHS/United States},
  issn = {1549-5469 (Electronic); 1088-9051 (Linking)},
  jid = {9518021},
  jt = {Genome research},
  language = {eng},
  lid = {10.1101/gr.107524.110 {$[$}doi{$]$}},
  lr = {20131023},
  mh = {Base Sequence; *Genome; Genomics/*methods; Sequence Analysis, DNA/*methods;
	*Software},
  mhda = {2010/12/25 06:00},
  oid = {NLM: PMC2928508},
  own = {NLM},
  phst = {2010/07/19 {$[$}aheadofprint{$]$}; 2010/08/04 {$[$}aheadofprint{$]$}},
  pii = {gr.107524.110},
  pl = {United States},
  pmc = {PMC2928508},
  pmid = {20644199},
  pst = {ppublish},
  pt = {Journal Article; Research Support, N.I.H., Extramural},
  sb = {IM},
  status = {MEDLINE}
}

@ARTICLE{OConnor2010,
  author = {O'Connor, Brian D. and Merriman, Barry and Nelson, Stanley F.},
  title = {SeqWare Query Engine: storing and searching sequence data in the
	cloud.},
  journal = {BMC Bioinformatics},
  year = {2010},
  volume = {11 Suppl 12},
  pages = {S2},
  __markedentry = {[bau04c:6]},
  abstract = {Since the introduction of next-generation DNA sequencers the rapid
	increase in sequencer throughput, and associated drop in costs, has
	resulted in more than a dozen human genomes being resequenced over
	the last few years. These efforts are merely a prelude for a future
	in which genome resequencing will be commonplace for both biomedical
	research and clinical applications. The dramatic increase in sequencer
	output strains all facets of computational infrastructure, especially
	databases and query interfaces. The advent of cloud computing, and
	a variety of powerful tools designed to process petascale datasets,
	provide a compelling solution to these ever increasing demands.In
	this work, we present the SeqWare Query Engine which has been created
	using modern cloud computing technologies and designed to support
	databasing information from thousands of genomes. Our backend implementation
	was built using the highly scalable, NoSQL HBase database from the
	Hadoop project. We also created a web-based frontend that provides
	both a programmatic and interactive query interface and integrates
	with widely used genome browsers and tools. Using the query engine,
	users can load and query variants (SNVs, indels, translocations,
	etc) with a rich level of annotations including coverage and functional
	consequences. As a proof of concept we loaded several whole genome
	datasets including the U87MG cell line. We also used a glioblastoma
	multiforme tumor/normal pair to both profile performance and provide
	an example of using the Hadoop MapReduce framework within the query
	engine. This software is open source and freely available from the
	SeqWare project (http://seqware.sourceforge.net).The SeqWare Query
	Engine provided an easy way to make the U87MG genome accessible to
	programmers and non-programmers alike. This enabled a faster and
	more open exploration of results, quicker tuning of parameters for
	heuristic variant calling filters, and a common data interface to
	simplify development of analytical tools. The range of data types
	supported, the ease of querying and integrating with existing tools,
	and the robust scalability of the underlying cloud-based technologies
	make SeqWare Query Engine a nature fit for storing and searching
	ever-growing genome sequence datasets.},
  doi = {10.1186/1471-2105-11-S12-S2},
  institution = {UNC Lineberger Comprehensive Cancer Center, University of North Carolina,
	Chapel Hill, NC 27599, USA.},
  keywords = {Databases, Nucleic Acid; Genome, Human; Genomics, methods; High-Throughput
	Nucleotide Sequencing; Humans; Sequence Analysis, DNA, methods; Software},
  language = {eng},
  medline-pst = {epublish},
  owner = {bau04c},
  pii = {1471-2105-11-S12-S2},
  pmid = {21210981},
  timestamp = {2014.01.08},
  url = {http://dx.doi.org/10.1186/1471-2105-11-S12-S2}
}

@ARTICLE{Oinn2004,
  author = {Oinn, Tom and Addis, Matthew and Ferris, Justin and Marvin, Darren
	and Senger, Martin and Greenwood, Mark and Carver, Tim and Glover,
	Kevin and Pocock, Matthew R. and Wipat, Anil and Li, Peter},
  title = {Taverna: a tool for the composition and enactment of bioinformatics
	workflows.},
  journal = {Bioinformatics},
  year = {2004},
  volume = {20},
  pages = {3045--3054},
  number = {17},
  month = {Nov},
  abstract = {In silico experiments in bioinformatics involve the co-ordinated use
	of computational tools and information repositories. A growing number
	of these resources are being made available with programmatic access
	in the form of Web services. Bioinformatics scientists will need
	to orchestrate these Web services in workflows as part of their analyses.The
	Taverna project has developed a tool for the composition and enactment
	of bioinformatics workflows for the life sciences community. The
	tool includes a workbench application which provides a graphical
	user interface for the composition of workflows. These workflows
	are written in a new language called the simple conceptual unified
	flow language (Scufl), where by each step within a workflow represents
	one atomic task. Two examples are used to illustrate the ease by
	which in silico experiments can be represented as Scufl workflows
	using the workbench application.},
  bdsk-url-1 = {http://dx.doi.org/10.1093/bioinformatics/bth361},
  doi = {10.1093/bioinformatics/bth361},
  institution = {EMBL European Bioinformatics Institute, Hinxton, Cambridge, CB10
	1SD, UK.},
  keywords = {Computational Biology, methods; Computer Communication Networks; Computer
	Graphics; Database Management Systems; Information Storage and Retrieval,
	methods; Internet; Online Systems; Software; Software Design; User-Computer
	Interface},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {bth361},
  pmid = {15201187},
  timestamp = {2013.08.28},
  url = {http://dx.doi.org/10.1093/bioinformatics/bth361}
}

@ARTICLE{Perez2007,
  author = {P\'erez, Fernando and Granger, Brian E.},
  title = {{IP}ython: a {S}ystem for {I}nteractive {S}cientific {C}omputing},
  journal = {{C}omput. {S}ci. {E}ng.},
  year = {2007},
  volume = {9},
  pages = {21-29},
  number = {3},
  month = may,
  bdsk-url-1 = {http://ipython.org},
  url = {http://ipython.org}
}

@ARTICLE{Sadedin2012,
  author = {Sadedin, Simon P. and Pope, Bernard and Oshlack, Alicia},
  title = {Bpipe: a tool for running and managing bioinformatics pipelines.},
  journal = {Bioinformatics},
  year = {2012},
  volume = {28},
  pages = {1525--1526},
  number = {11},
  month = {Jun},
  abstract = {Bpipe is a simple, dedicated programming language for defining and
	executing bioinformatics pipelines. It specializes in enabling users
	to turn existing pipelines based on shell scripts or command line
	tools into highly flexible, adaptable and maintainable workflows
	with a minimum of effort. Bpipe ensures that pipelines execute in
	a controlled and repeatable fashion and keeps audit trails and logs
	to ensure that experimental results are reproducible. Requiring only
	Java as a dependency, Bpipe is fully self-contained and cross-platform,
	making it very easy to adopt and deploy into existing environments.
	Availability and implementation: Bpipe is freely available from http://bpipe.org
	under a BSD License.},
  bdsk-url-1 = {http://dx.doi.org/10.1093/bioinformatics/bts167},
  doi = {10.1093/bioinformatics/bts167},
  institution = {Murdoch Childrens Research Institute, Royal Children's Hospital,
	Flemington Road, Parkville, Victoria 3052, Australia. simon.sadedin@mcri.edu.au},
  keywords = {Computational Biology, instrumentation/methods; Programming Languages;
	Software; Workflow},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {bts167},
  pmid = {22500002},
  timestamp = {2013.08.23},
  url = {http://dx.doi.org/10.1093/bioinformatics/bts167}
}

@ARTICLE{Tsirigos2012,
  author = {Tsirigos, Aristotelis and Haiminen, Niina and Bilal, Erhan and Utro,
	Filippo},
  title = {GenomicTools: a computational platform for developing high-throughput
	analytics in genomics.},
  journal = {Bioinformatics},
  year = {2012},
  volume = {28},
  pages = {282--283},
  number = {2},
  month = {Jan},
  abstract = {Recent advances in sequencing technology have resulted in the dramatic
	increase of sequencing data, which, in turn, requires efficient management
	of computational resources, such as computing time, memory requirements
	as well as prototyping of computational pipelines.We present GenomicTools,
	a flexible computational platform, comprising both a command-line
	set of tools and a C++ API, for the analysis and manipulation of
	high-throughput sequencing data such as DNA-seq, RNA-seq, ChIP-seq
	and MethylC-seq. GenomicTools implements a variety of mathematical
	operations between sets of genomic regions thereby enabling the prototyping
	of computational pipelines that can address a wide spectrum of tasks
	ranging from pre-processing and quality control to meta-analyses.
	Additionally, the GenomicTools platform is designed to analyze large
	datasets of any size by minimizing memory requirements. In practical
	applications, where comparable, GenomicTools outperforms existing
	tools in terms of both time and memory usage.The GenomicTools platform
	(version 2.0.0) was implemented in C++. The source code, documentation,
	user manual, example datasets and scripts are available online at
	http://code.google.com/p/ibm-cbc-genomic-tools.},
  bdsk-url-1 = {http://dx.doi.org/10.1093/bioinformatics/btr646},
  doi = {10.1093/bioinformatics/btr646},
  institution = {Computational Biology Center, IBM T.J. Watson Research Center, Yorktown
	Heights, NY 10598, USA. atsirigo@us.ibm.com},
  keywords = {Computational Biology, methods; Genome, Human; Genomics, methods;
	High-Throughput Nucleotide Sequencing; Humans; Software},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {btr646},
  pmid = {22113082},
  timestamp = {2013.08.23},
  url = {http://dx.doi.org/10.1093/bioinformatics/btr646}
}

@ARTICLE{Zhao2012,
  author = {Zhao, Yongbing and Wu, Jiayan and Yang, Junhui and Sun, Shixiang
	and Xiao, Jingfa and Yu, Jun},
  title = {PGAP: pan-genomes analysis pipeline.},
  journal = {Bioinformatics},
  year = {2012},
  volume = {28},
  pages = {416--418},
  number = {3},
  month = {Feb},
  abstract = {With the rapid development of DNA sequencing technology, increasing
	bacteria genome data enable the biologists to dig the evolutionary
	and genetic information of prokaryotic species from pan-genome sight.
	Therefore, the high-efficiency pipelines for pan-genome analysis
	are mostly needed. We have developed a new pan-genome analysis pipeline
	(PGAP), which can perform five analytic functions with only one command,
	including cluster analysis of functional genes, pan-genome profile
	analysis, genetic variation analysis of functional genes, species
	evolution analysis and function enrichment analysis of gene clusters.
	PGAP's performance has been evaluated on 11 Streptococcus pyogenes
	strains.PGAP is developed with Perl script on the Linux Platform
	and the package is freely available from http://pgap.sf.net.junyu@big.ac.cn;
	xiaojingfa@big.ac.cnSupplementary data are available at Bioinformatics
	online.},
  bdsk-url-1 = {http://dx.doi.org/10.1093/bioinformatics/btr655},
  doi = {10.1093/bioinformatics/btr655},
  institution = {CAS Key Laboratory of Genome Sciences and Information, Beijing Institute
	of Genomics, Chinese Academy of Sciences, Beijing 100029, People's
	Republic of China.},
  keywords = {Algorithms; Genome, Bacterial; Software; Streptococcus pyogenes, classification/genetics},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {btr655},
  pmid = {22130594},
  timestamp = {2013.08.23},
  url = {http://dx.doi.org/10.1093/bioinformatics/btr655}
}

@ARTICLE{Rubio-Camarillo2013,
  title = {RUbioSeq: a suite of parallelized pipelines to automate exome variation
	and bisulfite-seq analyses.},
  journal = {Bioinformatics},
  year = {2013},
  volume = {29},
  pages = {1687--1689},
  number = {13},
  month = {Jul},
  abstract = {RUbioSeq has been developed to facilitate the primary and secondary
	analysis of re-sequencing projects by providing an integrated software
	suite of parallelized pipelines to detect exome variants (single-nucleotide
	variants and copy number variations) and to perform bisulfite-seq
	analyses automatically. RUbioSeq's variant analysis results have
	been already validated and published.http://rubioseq.sourceforge.net/.},
  bdsk-url-1 = {http://dx.doi.org/10.1093/bioinformatics/btt203},
  doi = {10.1093/bioinformatics/btt203},
  institution = {Structural Computational Biology Group, Structural Biology and Biocomputing
	Programme, Spanish National Cancer Research Centre, 28029 Madrid,
	Spain. mrubioc@cnio.es},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {btt203},
  pmid = {23630175},
  timestamp = {2013.08.30},
  url = {http://dx.doi.org/10.1093/bioinformatics/btt203}
}

@ARTICLE{Althammer2011,
  title = {Pyicos: a versatile toolkit for the analysis of high-throughput sequencing
	data.},
  journal = {Bioinformatics},
  year = {2011},
  volume = {27},
  pages = {3333--3340},
  number = {24},
  month = {Dec},
  abstract = {High-throughput sequencing (HTS) has revolutionized gene regulation
	studies and is now fundamental for the detection of protein-DNA and
	protein-RNA binding, as well as for measuring RNA expression. With
	increasing variety and sequencing depth of HTS datasets, the need
	for more flexible and memory-efficient tools to analyse them is growing.We
	describe Pyicos, a powerful toolkit for the analysis of mapped reads
	from diverse HTS experiments: ChIP-Seq, either punctuated or broad
	signals, CLIP-Seq and RNA-Seq. We prove the effectiveness of Pyicos
	to select for significant signals and show that its accuracy is comparable
	and sometimes superior to that of methods specifically designed for
	each particular type of experiment. Pyicos facilitates the analysis
	of a variety of HTS datatypes through its flexibility and memory
	efficiency, providing a useful framework for data integration into
	models of regulatory genomics.Open-source software, with tutorials
	and protocol files, is available at http://regulatorygenomics.upf.edu/pyicos
	or as a Galaxy server at http://regulatorygenomics.upf.edu/galaxyeduardo.eyras@upf.eduSupplementary
	data are available at Bioinformatics online.},
  bdsk-url-1 = {http://dx.doi.org/10.1093/bioinformatics/btr570},
  doi = {10.1093/bioinformatics/btr570},
  institution = {Universitat Pompeu Fabra, E08003 Barcelona, Spain.},
  keywords = {Chromatin Immunoprecipitation; Computational Biology, methods; Computers;
	Gene Expression Regulation; High-Throughput Nucleotide Sequencing,
	methods; Sequence Analysis, RNA, methods; Software},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {btr570},
  pmid = {21994224},
  timestamp = {2013.08.23},
  url = {http://dx.doi.org/10.1093/bioinformatics/btr570}
}

@ARTICLE{Blanca2011,
  title = {ngs\_backbone: a pipeline for read cleaning, mapping and SNP calling
	using next generation sequence.},
  journal = {BMC Genomics},
  year = {2011},
  volume = {12},
  pages = {285},
  abstract = {The possibilities offered by next generation sequencing (NGS) platforms
	are revolutionizing biotechnological laboratories. Moreover, the
	combination of NGS sequencing and affordable high-throughput genotyping
	technologies is facilitating the rapid discovery and use of SNPs
	in non-model species. However, this abundance of sequences and polymorphisms
	creates new software needs. To fulfill these needs, we have developed
	a powerful, yet easy-to-use application.The ngs_backbone software
	is a parallel pipeline capable of analyzing Sanger, 454, Illumina
	and SOLiD (Sequencing by Oligonucleotide Ligation and Detection)
	sequence reads. Its main supported analyses are: read cleaning, transcriptome
	assembly and annotation, read mapping and single nucleotide polymorphism
	(SNP) calling and selection. In order to build a truly useful tool,
	the software development was paired with a laboratory experiment.
	All public tomato Sanger EST reads plus 14.2 million Illumina reads
	were employed to test the tool and predict polymorphism in tomato.
	The cleaned reads were mapped to the SGN tomato transcriptome obtaining
	a coverage of 4.2 for Sanger and 8.5 for Illumina. 23,360 single
	nucleotide variations (SNVs) were predicted. A total of 76 SNVs were
	experimentally validated, and 85\% were found to be real.ngs_backbone
	is a new software package capable of analyzing sequences produced
	by NGS technologies and predicting SNVs with great accuracy. In our
	tomato example, we created a highly polymorphic collection of SNVs
	that will be a useful resource for tomato researchers and breeders.
	The software developed along with its documentation is freely available
	under the AGPL license and can be downloaded from http://bioinf.comav.upv.es/ngs_backbone/
	or http://github.com/JoseBlanca/franklin.},
  bdsk-url-1 = {http://dx.doi.org/10.1186/1471-2164-12-285},
  doi = {10.1186/1471-2164-12-285},
  keywords = {Algorithms; Computational Biology; Documentation; High-Throughput
	Nucleotide Sequencing, methods; Lycopersicon esculentum, genetics;
	Polymorphism, Single Nucleotide, genetics; Reproducibility of Results;
	Software; Statistics as Topic, methods},
  language = {eng},
  medline-pst = {epublish},
  owner = {bau04c},
  pii = {1471-2164-12-285},
  pmid = {21635747},
  timestamp = {2013.08.30},
  url = {http://dx.doi.org/10.1186/1471-2164-12-285}
}

@ARTICLE{Neron2009,
  title = {Mobyle: a new full web bioinformatics framework.},
  journal = {Bioinformatics},
  year = {2009},
  volume = {25},
  pages = {3005--3011},
  number = {22},
  month = {Nov},
  abstract = {For the biologist, running bioinformatics analyses involves a time-consuming
	management of data and tools. Users need support to organize their
	work, retrieve parameters and reproduce their analyses. They also
	need to be able to combine their analytic tools using a safe data
	flow software mechanism. Finally, given that scientific tools can
	be difficult to install, it is particularly helpful for biologists
	to be able to use these tools through a web user interface. However,
	providing a web interface for a set of tools raises the problem that
	a single web portal cannot offer all the existing and possible services:
	it is the user, again, who has to cope with data copy among a number
	of different services. A framework enabling portal administrators
	to build a network of cooperating services would therefore clearly
	be beneficial.We have designed a system, Mobyle, to provide a flexible
	and usable Web environment for defining and running bioinformatics
	analyses. It embeds simple yet powerful data management features
	that allow the user to reproduce analyses and to combine tools using
	a hierarchical typing system. Mobyle offers invocation of services
	distributed over remote Mobyle servers, thus enabling a federated
	network of curated bioinformatics portals without the user having
	to learn complex concepts or to install sophisticated software. While
	being focused on the end user, the Mobyle system also addresses the
	need, for the bioinfomatician, to automate remote services execution:
	PlayMOBY is a companion tool that automates the publication of BioMOBY
	web services, using Mobyle program definitions.The Mobyle system
	is distributed under the terms of the GNU GPLv2 on the project web
	site (http://bioweb2.pasteur.fr/projects/mobyle/). It is already
	deployed on three servers: http://mobyle.pasteur.fr, http://mobyle.rpbs.univ-paris-diderot.fr
	and http://lipm-bioinfo.toulouse.inra.fr/Mobyle. The PlayMOBY companion
	is distributed under the terms of the CeCILL license, and is available
	at http://lipm-bioinfo.toulouse.inra.fr/biomoby/PlayMOBY/.},
  bdsk-url-1 = {http://dx.doi.org/10.1093/bioinformatics/btp493},
  doi = {10.1093/bioinformatics/btp493},
  keywords = {Computational Biology, methods; Databases, Factual; Internet; Software;
	User-Computer Interface},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {btp493},
  pmid = {19689959},
  timestamp = {2013.08.28},
  url = {http://dx.doi.org/10.1093/bioinformatics/btp493}
}

@comment{jabref-meta: selector_review:}

@comment{jabref-meta: selector_publisher:}

@comment{jabref-meta: selector_author:}

@comment{jabref-meta: selector_journal:}

@comment{jabref-meta: selector_keywords:}

