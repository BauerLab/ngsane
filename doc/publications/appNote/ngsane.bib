% This file was created with JabRef 2.8.
% Encoding: ISO8859_1

@ARTICLE{Althammer2011,
  author = {Althammer, Sonja and González-Vallinas, Juan and Ballaré, Cecilia
	and Beato, Miguel and Eyras, Eduardo},
  title = {Pyicos: a versatile toolkit for the analysis of high-throughput sequencing
	data.},
  journal = {Bioinformatics},
  year = {2011},
  volume = {27},
  pages = {3333--3340},
  number = {24},
  month = {Dec},
  abstract = {High-throughput sequencing (HTS) has revolutionized gene regulation
	studies and is now fundamental for the detection of protein-DNA and
	protein-RNA binding, as well as for measuring RNA expression. With
	increasing variety and sequencing depth of HTS datasets, the need
	for more flexible and memory-efficient tools to analyse them is growing.We
	describe Pyicos, a powerful toolkit for the analysis of mapped reads
	from diverse HTS experiments: ChIP-Seq, either punctuated or broad
	signals, CLIP-Seq and RNA-Seq. We prove the effectiveness of Pyicos
	to select for significant signals and show that its accuracy is comparable
	and sometimes superior to that of methods specifically designed for
	each particular type of experiment. Pyicos facilitates the analysis
	of a variety of HTS datatypes through its flexibility and memory
	efficiency, providing a useful framework for data integration into
	models of regulatory genomics.Open-source software, with tutorials
	and protocol files, is available at http://regulatorygenomics.upf.edu/pyicos
	or as a Galaxy server at http://regulatorygenomics.upf.edu/galaxyeduardo.eyras@upf.eduSupplementary
	data are available at Bioinformatics online.},
  doi = {10.1093/bioinformatics/btr570},
  institution = {Universitat Pompeu Fabra, E08003 Barcelona, Spain.},
  keywords = {Chromatin Immunoprecipitation; Computational Biology, methods; Computers;
	Gene Expression Regulation; High-Throughput Nucleotide Sequencing,
	methods; Sequence Analysis, RNA, methods; Software},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {btr570},
  pmid = {21994224},
  timestamp = {2013.08.23},
  url = {http://dx.doi.org/10.1093/bioinformatics/btr570}
}

@ARTICLE{Angiuoli2011,
  author = {Angiuoli, Samuel V. and Matalka, Malcolm and Gussman, Aaron and Galens,
	Kevin and Vangala, Mahesh and Riley, David R. and Arze, Cesar and
	White, James R. and White, Owen and Fricke, W Florian},
  title = {CloVR: a virtual machine for automated and portable sequence analysis
	from the desktop using cloud computing.},
  journal = {BMC Bioinformatics},
  year = {2011},
  volume = {12},
  pages = {356},
  __markedentry = {[bau04c:]},
  abstract = {Next-generation sequencing technologies have decentralized sequence
	acquisition, increasing the demand for new bioinformatics tools that
	are easy to use, portable across multiple platforms, and scalable
	for high-throughput applications. Cloud computing platforms provide
	on-demand access to computing infrastructure over the Internet and
	can be used in combination with custom built virtual machines to
	distribute pre-packaged with pre-configured software.We describe
	the Cloud Virtual Resource, CloVR, a new desktop application for
	push-button automated sequence analysis that can utilize cloud computing
	resources. CloVR is implemented as a single portable virtual machine
	(VM) that provides several automated analysis pipelines for microbial
	genomics, including 16S, whole genome and metagenome sequence analysis.
	The CloVR VM runs on a personal computer, utilizes local computer
	resources and requires minimal installation, addressing key challenges
	in deploying bioinformatics workflows. In addition CloVR supports
	use of remote cloud computing resources to improve performance for
	large-scale sequence processing. In a case study, we demonstrate
	the use of CloVR to automatically process next-generation sequencing
	data on multiple cloud computing platforms.The CloVR VM and associated
	architecture lowers the barrier of entry for utilizing complex analysis
	protocols on both local single- and multi-core computers and cloud
	systems for high throughput data processing.},
  doi = {10.1186/1471-2105-12-356},
  institution = {Institute for Genome Sciences (IGS), University of Maryland School
	of Medicine, Baltimore, Maryland, USA. angiuoli@umiacs.umd.edu},
  keywords = {Computational Biology; Computers; Genomics; High-Throughput Nucleotide
	Sequencing; Internet; Sequence Analysis, DNA; Software},
  language = {eng},
  medline-pst = {epublish},
  owner = {bau04c},
  pii = {1471-2105-12-356},
  pmid = {21878105},
  timestamp = {2013.08.28},
  url = {http://dx.doi.org/10.1186/1471-2105-12-356}
}

@ARTICLE{Auer2010,
  author = {Auer, Paul L. and Doerge, R. W.},
  title = {Statistical design and analysis of RNA sequencing data.},
  journal = {Genetics},
  year = {2010},
  volume = {185},
  pages = {405--416},
  number = {2},
  month = {Jun},
  abstract = {Next-generation sequencing technologies are quickly becoming the preferred
	approach for characterizing and quantifying entire genomes. Even
	though data produced from these technologies are proving to be the
	most informative of any thus far, very little attention has been
	paid to fundamental design aspects of data collection and analysis,
	namely sampling, randomization, replication, and blocking. We discuss
	these concepts in an RNA sequencing framework. Using simulations
	we demonstrate the benefits of collecting replicated RNA sequencing
	data according to well known statistical designs that partition the
	sources of biological and technical variation. Examples of these
	designs and their corresponding models are presented with the goal
	of testing differential expression.},
  doi = {10.1534/genetics.110.114983},
  institution = {Department of Statistics, Purdue University, West Lafayette, Indiana
	47907, USA.},
  keywords = {Base Sequence; Clinical Laboratory Techniques; Research Design},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {genetics.110.114983},
  pmid = {20439781},
  timestamp = {2013.08.27},
  url = {http://dx.doi.org/10.1534/genetics.110.114983}
}

@ARTICLE{Brouwer2012,
  author = {Brouwer, R W W. and {van den Hout}, M C G N. and Grosveld, F. G.
	and {van Ijcken}, W F J.},
  title = {NARWHAL, a primary analysis pipeline for NGS data.},
  journal = {Bioinformatics},
  year = {2012},
  volume = {28},
  pages = {284--285},
  number = {2},
  month = {Jan},
  abstract = {The NARWHAL software pipeline has been developed to automate the primary
	analysis of Illumina sequencing data. This pipeline combines a new
	and flexible de-multiplexing tool with open-source aligners and automated
	quality assessment. The entire pipeline can be run using only one
	simple sample-sheet for diverse sequencing applications. NARWHAL
	creates a sample-oriented data structure and outperforms existing
	tools in speed.https://trac.nbic.nl/narwhal/.},
  doi = {10.1093/bioinformatics/btr613},
  institution = {Center for Biomics, Department of Cell Biology, Erasmus Medical Center,
	Rotterdam, The Netherlands.},
  keywords = {High-Throughput Nucleotide Sequencing; Humans; Oligonucleotide Array
	Sequence Analysis; Sequence Alignment; Sequence Analysis, DNA, methods;
	Software},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {btr613},
  pmid = {22072383},
  timestamp = {2013.08.23},
  url = {http://dx.doi.org/10.1093/bioinformatics/btr613}
}

@ARTICLE{Cieslik2011,
  author = {Cieslik, Marcin and Mura, Cameron},
  title = {A lightweight, flow-based toolkit for parallel and distributed bioinformatics
	pipelines.},
  journal = {BMC Bioinformatics},
  year = {2011},
  volume = {12},
  pages = {61},
  abstract = {Bioinformatic analyses typically proceed as chains of data-processing
	tasks. A pipeline, or 'workflow', is a well-defined protocol, with
	a specific structure defined by the topology of data-flow interdependencies,
	and a particular functionality arising from the data transformations
	applied at each step. In computer science, the dataflow programming
	(DFP) paradigm defines software systems constructed in this manner,
	as networks of message-passing components. Thus, bioinformatic workflows
	can be naturally mapped onto DFP concepts.To enable the flexible
	creation and execution of bioinformatics dataflows, we have written
	a modular framework for parallel pipelines in Python ('PaPy'). A
	PaPy workflow is created from re-usable components connected by data-pipes
	into a directed acyclic graph, which together define nested higher-order
	map functions. The successive functional transformations of input
	data are evaluated on flexibly pooled compute resources, either local
	or remote. Input items are processed in batches of adjustable size,
	all flowing one to tune the trade-off between parallelism and lazy-evaluation
	(memory consumption). An add-on module ('NuBio') facilitates the
	creation of bioinformatics workflows by providing domain specific
	data-containers (e.g., for biomolecular sequences, alignments, structures)
	and functionality (e.g., to parse/write standard file formats).PaPy
	offers a modular framework for the creation and deployment of parallel
	and distributed data-processing workflows. Pipelines derive their
	functionality from user-written, data-coupled components, so PaPy
	also can be viewed as a lightweight toolkit for extensible, flow-based
	bioinformatics data-processing. The simplicity and flexibility of
	distributed PaPy pipelines may help users bridge the gap between
	traditional desktop/workstation and grid computing. PaPy is freely
	distributed as open-source Python code at http://muralab.org/PaPy,
	and includes extensive documentation and annotated usage examples.},
  doi = {10.1186/1471-2105-12-61},
  institution = {Department of Chemistry, University of Virginia, Charlottesville,
	VA 22904-4319, USA.},
  keywords = {Computational Biology, methods; Computing Methodologies; Programming
	Languages; Software; Workflow},
  language = {eng},
  medline-pst = {epublish},
  owner = {bau04c},
  pii = {1471-2105-12-61},
  pmid = {21352538},
  timestamp = {2013.08.23},
  url = {http://dx.doi.org/10.1186/1471-2105-12-61}
}

@ARTICLE{Goecks2010,
  author = {Goecks, Jeremy and Nekrutenko, Anton and Taylor, James and , Galaxy
	Team},
  title = {Galaxy: a comprehensive approach for supporting accessible, reproducible,
	and transparent computational research in the life sciences.},
  journal = {Genome Biol},
  year = {2010},
  volume = {11},
  pages = {R86},
  number = {8},
  abstract = {Increased reliance on computational approaches in the life sciences
	has revealed grave concerns about how accessible and reproducible
	computation-reliant results truly are. Galaxy http://usegalaxy.org,
	an open web-based platform for genomic research, addresses these
	problems. Galaxy automatically tracks and manages data provenance
	and provides support for capturing the context and intent of computational
	methods. Galaxy Pages are interactive, web-based documents that provide
	users with a medium to communicate a complete computational analysis.},
  doi = {10.1186/gb-2010-11-8-r86},
  institution = {Department of Biology and Department of Mathematics and Computer
	Science, Emory University, 1510 Clifton Road NE, Atlanta, GA 30322,
	USA. jeremy.goecks@emory.edu},
  keywords = {Algorithms; Animals; Computational Biology, methods; Databases, Nucleic
	Acid; Genomics, methods; Humans; Internet},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {gb-2010-11-8-r86},
  pmid = {20738864},
  timestamp = {2013.08.23},
  url = {http://dx.doi.org/10.1186/gb-2010-11-8-r86}
}

@ARTICLE{Goodstadt2010,
  author = {Goodstadt, Leo},
  title = {Ruffus: a lightweight Python library for computational pipelines.},
  journal = {Bioinformatics},
  year = {2010},
  volume = {26},
  pages = {2778--2779},
  number = {21},
  month = {Nov},
  abstract = {Computational pipelines are common place in scientific research. However,
	most of the resources for constructing pipelines are heavyweight
	systems with graphical user interfaces. Ruffus is a library for the
	creation of computational pipelines. Its lightweight and unobtrusive
	design recommends it for use even for the most trivial of analyses.
	At the same time, it is powerful enough to have been used for complex
	workflows involving more than 50 interdependent stages. Availability
	and implementation: Ruffus is written in python. Source code, a short
	tutorial, examples and a comprehensive user manual are freely available
	at http://www.ruffus.org.uk. The example program is available at
	http://www.ruffus.org.uk/examples/bioinformatics},
  doi = {10.1093/bioinformatics/btq524},
  institution = {Medical Research Council Functional Genomics Unit, Department of
	Physiology, Anatomy and Genetics, University of Oxford, Oxford, UK.
	ruffus@llew.org.uk},
  keywords = {Computational Biology, methods; Databases, Factual; Internet; Software},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {btq524},
  pmid = {20847218},
  timestamp = {2013.08.23},
  url = {http://dx.doi.org/10.1093/bioinformatics/btq524}
}

@ARTICLE{Halbritter2012,
  author = {Halbritter, Florian and Vaidya, Harsh J. and Tomlinson, Simon R.},
  title = {GeneProf: analysis of high-throughput sequencing experiments.},
  journal = {Nat Methods},
  year = {2012},
  volume = {9},
  pages = {7--8},
  number = {1},
  month = {Jan},
  doi = {10.1038/nmeth.1809},
  keywords = {Capillary Electrochromatography; Computational Biology, methods; Gene
	Expression; Sequence Alignment; Sequence Analysis, DNA, methods;
	Sequence Analysis, RNA, methods; Software},
  language = {eng},
  medline-pst = {epublish},
  owner = {bau04c},
  pii = {nmeth.1809},
  pmid = {22205509},
  timestamp = {2013.08.28},
  url = {http://dx.doi.org/10.1038/nmeth.1809}
}

@ARTICLE{Hoon2003,
  author = {Hoon, Shawn and Ratnapu, Kiran Kumar and Chia, Jer-Ming and Kumarasamy,
	Balamurugan and Juguang, Xiao and Clamp, Michele and Stabenau, Arne
	and Potter, Simon and Clarke, Laura and Stupka, Elia},
  title = {Biopipe: a flexible framework for protocol-based bioinformatics analysis.},
  journal = {Genome Res},
  year = {2003},
  volume = {13},
  pages = {1904--1915},
  number = {8},
  month = {Aug},
  abstract = {We identify several challenges facing bioinformatics analysis today.
	Firstly, to fulfill the promise of comparative studies, bioinformatics
	analysis will need to accommodate different sources of data residing
	in a federation of databases that, in turn, come in different formats
	and modes of accessibility. Secondly, the tsunami of data to be handled
	will require robust systems that enable bioinformatics analysis to
	be carried out in a parallel fashion. Thirdly, the ever-evolving
	state of bioinformatics presents new algorithms and paradigms in
	conducting analysis. This means that any bioinformatics framework
	must be flexible and generic enough to accommodate such changes.
	In addition, we identify the need for introducing an explicit protocol-based
	approach to bioinformatics analysis that will lend rigorousness to
	the analysis. This makes it easier for experimentation and replication
	of results by external parties. Biopipe is designed in an effort
	to meet these goals. It aims to allow researchers to focus on protocol
	design. At the same time, it is designed to work over a compute farm
	and thus provides high-throughput performance. A common exchange
	format that encapsulates the entire protocol in terms of the analysis
	modules, parameters, and data versions has been developed to provide
	a powerful way in which to distribute and reproduce results. This
	will enable researchers to discuss and interpret the data better
	as the once implicit assumptions are now explicitly defined within
	the Biopipe framework.},
  doi = {10.1101/gr.1363103},
  institution = {Institute of Molecular and Cell Biology, National University of Singapore,
	Singapore 117609.},
  keywords = {Amino Acid Sequence; Animals; Computational Biology, methods; Databases,
	Protein; Drosophila Proteins, genetics; Humans; Phylogeny; Proteins,
	genetics; Research Design, trends; Software; Software Design; Takifugu,
	genetics},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {1363103},
  pmid = {12869579},
  timestamp = {2013.08.28},
  url = {http://dx.doi.org/10.1101/gr.1363103}
}

@ARTICLE{Koester2012,
  author = {Köster, Johannes and Rahmann, Sven},
  title = {Snakemake--a scalable bioinformatics workflow engine.},
  journal = {Bioinformatics},
  year = {2012},
  volume = {28},
  pages = {2520--2522},
  number = {19},
  month = {Oct},
  abstract = {Snakemake is a workflow engine that provides a readable Python-based
	workflow definition language and a powerful execution environment
	that scales from single-core workstations to compute clusters without
	modifying the workflow. It is the first system to support the use
	of automatically inferred multiple named wildcards (or variables)
	in input and output filenames.http://snakemake.googlecode.com.johannes.koester@uni-due.de.},
  doi = {10.1093/bioinformatics/bts480},
  institution = {Genome Informatics, Institute of Human Genetics, University of Duisburg-Essen
	and Paediatric Oncology, University Childrens Hospital, 45147 Essen,
	Germany. johannes.koester@uni-due.de},
  keywords = {Automatic Data Processing; Computational Biology, methods; Programming
	Languages; Software; Workflow},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {bts480},
  pmid = {22908215},
  timestamp = {2013.08.28},
  url = {http://dx.doi.org/10.1093/bioinformatics/bts480}
}

@ARTICLE{Langmead2009,
  author = {Langmead, Ben and Schatz, Michael C. and Lin, Jimmy and Pop, Mihai
	and Salzberg, Steven L.},
  title = {Searching for SNPs with cloud computing.},
  journal = {Genome Biol},
  year = {2009},
  volume = {10},
  pages = {R134},
  number = {11},
  __markedentry = {[bau04c:6]},
  abstract = {As DNA sequencing outpaces improvements in computer speed, there is
	a critical need to accelerate tasks like alignment and SNP calling.
	Crossbow is a cloud-computing software tool that combines the aligner
	Bowtie and the SNP caller SOAPsnp. Executing in parallel using Hadoop,
	Crossbow analyzes data comprising 38-fold coverage of the human genome
	in three hours using a 320-CPU cluster rented from a cloud computing
	service for about $85. Crossbow is available from http://bowtie-bio.sourceforge.net/crossbow/.},
  doi = {10.1186/gb-2009-10-11-r134},
  institution = {Department of Biostatistics, Johns Hopkins Bloomberg School of Public
	Health, 615 North Wolfe Street, Baltimore, Maryland 21205, USA. blangmea@jhsph.edu},
  keywords = {Algorithms; Alleles; Chromosomes, Human, Pair 22, genetics; Chromosomes,
	Human, X, genetics; Chromosomes, ultrastructure; Computational Biology,
	methods; Computer Simulation; Computers; Heterozygote; Humans; Models,
	Genetic; Polymorphism, Single Nucleotide; Sequence Analysis, DNA;
	Software},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {gb-2009-10-11-r134},
  pmid = {19930550},
  timestamp = {2013.08.28},
  url = {http://dx.doi.org/10.1186/gb-2009-10-11-r134}
}

@ARTICLE{Lindenbaum2011,
  author = {Lindenbaum, Pierre and {Le Scouarnec}, Solena and Portero, Vincent
	and Redon, Richard},
  title = {Knime4Bio: a set of custom nodes for the interpretation of next-generation
	sequencing data with KNIME.},
  journal = {Bioinformatics},
  year = {2011},
  volume = {27},
  pages = {3200--3201},
  number = {22},
  month = {Nov},
  abstract = {Analysing large amounts of data generated by next-generation sequencing
	(NGS) technologies is difficult for researchers or clinicians without
	computational skills. They are often compelled to delegate this task
	to computer biologists working with command line utilities. The availability
	of easy-to-use tools will become essential with the generalization
	of NGS in research and diagnosis. It will enable investigators to
	handle much more of the analysis. Here, we describe Knime4Bio, a
	set of custom nodes for the KNIME (The Konstanz Information Miner)
	interactive graphical workbench, for the interpretation of large
	biological datasets. We demonstrate that this tool can be utilized
	to quickly retrieve previously published scientific findings.},
  doi = {10.1093/bioinformatics/btr554},
  institution = {Institut du thorax, Inserm UMR 915, Centre Hospitalier Universitaire
	de Nantes, 44000 Nantes, France.},
  keywords = {Hajdu-Cheney Syndrome, genetics; High-Throughput Nucleotide Sequencing,
	methods; Humans; Software},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {btr554},
  pmid = {21984761},
  timestamp = {2013.08.23},
  url = {http://dx.doi.org/10.1093/bioinformatics/btr554}
}

@ARTICLE{Linke2011,
  author = {Linke, Burkhard and Giegerich, Robert and Goesmann, Alexander},
  title = {Conveyor: a workflow engine for bioinformatic analyses.},
  journal = {Bioinformatics},
  year = {2011},
  volume = {27},
  pages = {903--911},
  number = {7},
  month = {Apr},
  abstract = {The rapidly increasing amounts of data available from new high-throughput
	methods have made data processing without automated pipelines infeasible.
	As was pointed out in several publications, integration of data and
	analytic resources into workflow systems provides a solution to this
	problem, simplifying the task of data analysis. Various applications
	for defining and running workflows in the field of bioinformatics
	have been proposed and published, e.g. Galaxy, Mobyle, Taverna, Pegasus
	or Kepler. One of the main aims of such workflow systems is to enable
	scientists to focus on analysing their datasets instead of taking
	care for data management, job management or monitoring the execution
	of computational tasks. The currently available workflow systems
	achieve this goal, but fundamentally differ in their way of executing
	workflows.We have developed the Conveyor software library, a multitiered
	generic workflow engine for composition, execution and monitoring
	of complex workflows. It features an open, extensible system architecture
	and concurrent program execution to exploit resources available on
	modern multicore CPU hardware. It offers the ability to build complex
	workflows with branches, loops and other control structures. Two
	example use cases illustrate the application of the versatile Conveyor
	engine to common bioinformatics problems.The Conveyor application
	including client and server are available at http://conveyor.cebitec.uni-bielefeld.de.},
  doi = {10.1093/bioinformatics/btr040},
  institution = {Bioinformatics Resource Faciliy, Center for Biotechnology and Faculty
	of Technology, Bielefeld University, 33615 Bielefeld, Germany. blinke@ceBiTec.Uni-Bielefeld.De},
  keywords = {Computational Biology; Escherichia coli, genetics; Genome, Bacterial;
	Genomics; Molecular Sequence Annotation; Software; Workflow},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {btr040},
  pmid = {21278189},
  timestamp = {2013.08.23},
  url = {http://dx.doi.org/10.1093/bioinformatics/btr040}
}

@ARTICLE{McCoy2013,
  author = {McCoy, Connor O. and Gallagher, Aaron and Hoffman, Noah G. and Matsen,
	Frederick A.},
  title = {Nestly--a framework for running software with nested parameter choices
	and aggregating results.},
  journal = {Bioinformatics},
  year = {2013},
  volume = {29},
  pages = {387--388},
  number = {3},
  month = {Feb},
  abstract = {The execution of a software application or pipeline using various
	combinations of parameters and inputs is a common task in bioinformatics.
	In the absence of a specialized tool to organize, streamline and
	formalize this process, scientists must write frequently complex
	scripts to perform these tasks. We present nestly, a Python package
	to facilitate running tools with nested combinations of parameters
	and inputs. nestly provides three components. First, a module to
	build nested directory structures corresponding to choices of parameters.
	Second, the nestrun script to run a given command using each set
	of parameter choices. Third, the nestagg script to aggregate results
	of the individual runs into a CSV file, as well as support for more
	complex aggregation. We also include a module for easily specifying
	nested dependencies for the SCons build tool, enabling incremental
	builds.Source, documentation and tutorial examples are available
	at http://github.com/fhcrc/nestly. nestly can be installed from the
	Python Package Index via pip; it is open source (MIT license).},
  doi = {10.1093/bioinformatics/bts696},
  institution = {Program in Computational Biology, Fred Hutchinson Cancer Research
	Center, Seattle, WA 98109, USA. cmccoy@fhcrc.org},
  keywords = {Algorithms; Computational Biology; Humans; Software},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {bts696},
  pmid = {23220574},
  timestamp = {2013.08.28},
  url = {http://dx.doi.org/10.1093/bioinformatics/bts696}
}

@ARTICLE{Neron2009,
  author = {Néron, Bertrand and Ménager, Hervé and Maufrais, Corinne and Joly,
	Nicolas and Maupetit, Julien and Letort, Sébastien and Carrere, Sébastien
	and Tuffery, Pierre and Letondal, Catherine},
  title = {Mobyle: a new full web bioinformatics framework.},
  journal = {Bioinformatics},
  year = {2009},
  volume = {25},
  pages = {3005--3011},
  number = {22},
  month = {Nov},
  abstract = {For the biologist, running bioinformatics analyses involves a time-consuming
	management of data and tools. Users need support to organize their
	work, retrieve parameters and reproduce their analyses. They also
	need to be able to combine their analytic tools using a safe data
	flow software mechanism. Finally, given that scientific tools can
	be difficult to install, it is particularly helpful for biologists
	to be able to use these tools through a web user interface. However,
	providing a web interface for a set of tools raises the problem that
	a single web portal cannot offer all the existing and possible services:
	it is the user, again, who has to cope with data copy among a number
	of different services. A framework enabling portal administrators
	to build a network of cooperating services would therefore clearly
	be beneficial.We have designed a system, Mobyle, to provide a flexible
	and usable Web environment for defining and running bioinformatics
	analyses. It embeds simple yet powerful data management features
	that allow the user to reproduce analyses and to combine tools using
	a hierarchical typing system. Mobyle offers invocation of services
	distributed over remote Mobyle servers, thus enabling a federated
	network of curated bioinformatics portals without the user having
	to learn complex concepts or to install sophisticated software. While
	being focused on the end user, the Mobyle system also addresses the
	need, for the bioinfomatician, to automate remote services execution:
	PlayMOBY is a companion tool that automates the publication of BioMOBY
	web services, using Mobyle program definitions.The Mobyle system
	is distributed under the terms of the GNU GPLv2 on the project web
	site (http://bioweb2.pasteur.fr/projects/mobyle/). It is already
	deployed on three servers: http://mobyle.pasteur.fr, http://mobyle.rpbs.univ-paris-diderot.fr
	and http://lipm-bioinfo.toulouse.inra.fr/Mobyle. The PlayMOBY companion
	is distributed under the terms of the CeCILL license, and is available
	at http://lipm-bioinfo.toulouse.inra.fr/biomoby/PlayMOBY/.},
  doi = {10.1093/bioinformatics/btp493},
  institution = {Groupe Logiciels et Banques de Données, Institut Pasteur, 28, rue
	du Dr Roux, 75724 Paris Cedex, France. mobyle-support@pasteur.fr},
  keywords = {Computational Biology, methods; Databases, Factual; Internet; Software;
	User-Computer Interface},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {btp493},
  pmid = {19689959},
  timestamp = {2013.08.28},
  url = {http://dx.doi.org/10.1093/bioinformatics/btp493}
}

@ARTICLE{Oinn2004,
  author = {Oinn, Tom and Addis, Matthew and Ferris, Justin and Marvin, Darren
	and Senger, Martin and Greenwood, Mark and Carver, Tim and Glover,
	Kevin and Pocock, Matthew R. and Wipat, Anil and Li, Peter},
  title = {Taverna: a tool for the composition and enactment of bioinformatics
	workflows.},
  journal = {Bioinformatics},
  year = {2004},
  volume = {20},
  pages = {3045--3054},
  number = {17},
  month = {Nov},
  abstract = {In silico experiments in bioinformatics involve the co-ordinated use
	of computational tools and information repositories. A growing number
	of these resources are being made available with programmatic access
	in the form of Web services. Bioinformatics scientists will need
	to orchestrate these Web services in workflows as part of their analyses.The
	Taverna project has developed a tool for the composition and enactment
	of bioinformatics workflows for the life sciences community. The
	tool includes a workbench application which provides a graphical
	user interface for the composition of workflows. These workflows
	are written in a new language called the simple conceptual unified
	flow language (Scufl), where by each step within a workflow represents
	one atomic task. Two examples are used to illustrate the ease by
	which in silico experiments can be represented as Scufl workflows
	using the workbench application.},
  doi = {10.1093/bioinformatics/bth361},
  institution = {EMBL European Bioinformatics Institute, Hinxton, Cambridge, CB10
	1SD, UK.},
  keywords = {Computational Biology, methods; Computer Communication Networks; Computer
	Graphics; Database Management Systems; Information Storage and Retrieval,
	methods; Internet; Online Systems; Software; Software Design; User-Computer
	Interface},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {bth361},
  pmid = {15201187},
  timestamp = {2013.08.28},
  url = {http://dx.doi.org/10.1093/bioinformatics/bth361}
}

@ARTICLE{Perez2007,
  author = {P\'erez, Fernando and Granger, Brian E.},
  title = {{IP}ython: a {S}ystem for {I}nteractive {S}cientific {C}omputing},
  journal = {{C}omput. {S}ci. {E}ng.},
  year = {2007},
  volume = {9},
  pages = {21-29},
  number = {3},
  month = may,
  url = {http://ipython.org}
}

@ARTICLE{Sadedin2012,
  author = {Sadedin, Simon P. and Pope, Bernard and Oshlack, Alicia},
  title = {Bpipe: a tool for running and managing bioinformatics pipelines.},
  journal = {Bioinformatics},
  year = {2012},
  volume = {28},
  pages = {1525--1526},
  number = {11},
  month = {Jun},
  abstract = {Bpipe is a simple, dedicated programming language for defining and
	executing bioinformatics pipelines. It specializes in enabling users
	to turn existing pipelines based on shell scripts or command line
	tools into highly flexible, adaptable and maintainable workflows
	with a minimum of effort. Bpipe ensures that pipelines execute in
	a controlled and repeatable fashion and keeps audit trails and logs
	to ensure that experimental results are reproducible. Requiring only
	Java as a dependency, Bpipe is fully self-contained and cross-platform,
	making it very easy to adopt and deploy into existing environments.
	Availability and implementation: Bpipe is freely available from http://bpipe.org
	under a BSD License.},
  doi = {10.1093/bioinformatics/bts167},
  institution = {Murdoch Childrens Research Institute, Royal Children's Hospital,
	Flemington Road, Parkville, Victoria 3052, Australia. simon.sadedin@mcri.edu.au},
  keywords = {Computational Biology, instrumentation/methods; Programming Languages;
	Software; Workflow},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {bts167},
  pmid = {22500002},
  timestamp = {2013.08.23},
  url = {http://dx.doi.org/10.1093/bioinformatics/bts167}
}

@ARTICLE{Tsirigos2012,
  author = {Tsirigos, Aristotelis and Haiminen, Niina and Bilal, Erhan and Utro,
	Filippo},
  title = {GenomicTools: a computational platform for developing high-throughput
	analytics in genomics.},
  journal = {Bioinformatics},
  year = {2012},
  volume = {28},
  pages = {282--283},
  number = {2},
  month = {Jan},
  abstract = {Recent advances in sequencing technology have resulted in the dramatic
	increase of sequencing data, which, in turn, requires efficient management
	of computational resources, such as computing time, memory requirements
	as well as prototyping of computational pipelines.We present GenomicTools,
	a flexible computational platform, comprising both a command-line
	set of tools and a C++ API, for the analysis and manipulation of
	high-throughput sequencing data such as DNA-seq, RNA-seq, ChIP-seq
	and MethylC-seq. GenomicTools implements a variety of mathematical
	operations between sets of genomic regions thereby enabling the prototyping
	of computational pipelines that can address a wide spectrum of tasks
	ranging from pre-processing and quality control to meta-analyses.
	Additionally, the GenomicTools platform is designed to analyze large
	datasets of any size by minimizing memory requirements. In practical
	applications, where comparable, GenomicTools outperforms existing
	tools in terms of both time and memory usage.The GenomicTools platform
	(version 2.0.0) was implemented in C++. The source code, documentation,
	user manual, example datasets and scripts are available online at
	http://code.google.com/p/ibm-cbc-genomic-tools.},
  doi = {10.1093/bioinformatics/btr646},
  institution = {Computational Biology Center, IBM T.J. Watson Research Center, Yorktown
	Heights, NY 10598, USA. atsirigo@us.ibm.com},
  keywords = {Computational Biology, methods; Genome, Human; Genomics, methods;
	High-Throughput Nucleotide Sequencing; Humans; Software},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {btr646},
  pmid = {22113082},
  timestamp = {2013.08.23},
  url = {http://dx.doi.org/10.1093/bioinformatics/btr646}
}

@ARTICLE{Zhao2012,
  author = {Zhao, Yongbing and Wu, Jiayan and Yang, Junhui and Sun, Shixiang
	and Xiao, Jingfa and Yu, Jun},
  title = {PGAP: pan-genomes analysis pipeline.},
  journal = {Bioinformatics},
  year = {2012},
  volume = {28},
  pages = {416--418},
  number = {3},
  month = {Feb},
  abstract = {With the rapid development of DNA sequencing technology, increasing
	bacteria genome data enable the biologists to dig the evolutionary
	and genetic information of prokaryotic species from pan-genome sight.
	Therefore, the high-efficiency pipelines for pan-genome analysis
	are mostly needed. We have developed a new pan-genome analysis pipeline
	(PGAP), which can perform five analytic functions with only one command,
	including cluster analysis of functional genes, pan-genome profile
	analysis, genetic variation analysis of functional genes, species
	evolution analysis and function enrichment analysis of gene clusters.
	PGAP's performance has been evaluated on 11 Streptococcus pyogenes
	strains.PGAP is developed with Perl script on the Linux Platform
	and the package is freely available from http://pgap.sf.net.junyu@big.ac.cn;
	xiaojingfa@big.ac.cnSupplementary data are available at Bioinformatics
	online.},
  doi = {10.1093/bioinformatics/btr655},
  institution = {CAS Key Laboratory of Genome Sciences and Information, Beijing Institute
	of Genomics, Chinese Academy of Sciences, Beijing 100029, People's
	Republic of China.},
  keywords = {Algorithms; Genome, Bacterial; Software; Streptococcus pyogenes, classification/genetics},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {btr655},
  pmid = {22130594},
  timestamp = {2013.08.23},
  url = {http://dx.doi.org/10.1093/bioinformatics/btr655}
}

@comment{jabref-meta: selector_review:}

@comment{jabref-meta: selector_publisher:}

@comment{jabref-meta: selector_author:}

@comment{jabref-meta: selector_journal:}

@comment{jabref-meta: selector_keywords:}

